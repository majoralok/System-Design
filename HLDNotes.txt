Load balancing Policy
	1. Round Robin
	2. Weighted Round Robin
	3. Least Connection
	4. Least Response Time
	5. IP based Hashing/Layer 4
	6. Consistent Hashing
	7. Dynamic Load Balancing: Run Time server load calculator to distribute requests.
	8. Geography based
	9. Content Based request grouping/segregation or Layer 7 based

Content Based LB:

• Web Applications: Layer 7 load balancing is commonly used for distributing HTTP and HTTPS traffic in web applications. It can route requests based on URL patterns, content types, or specific application features.
• Content Delivery Networks (CDNs): CDNs often leverage Layer 7 load balancing to efficiently distribute content by considering factors like content type, user location, and the type of device accessing the content.
• Application Gateways: Layer 7 load balancing is integral to application gateways and API gateways, ensuring that requests are directed to the appropriate backend service based on the nature of the request.


Heath Check vs Hear Beat:
Health Check is when load balance hit /health end point whereas in heart beat each server calls/send single to LB that its alive. (heartBeat Miss)


Uneven distribution of data is called skew.

Consistent Hashing:
http://highscalability.com/blog/2023/2/22/consistent-hashing-algorithm.html#:~:text=Consistent%20hashing%20implementation&text=The%20self%2Dbalancing%20binary%20search,%2C%20insert%2C%20and%20delete%20operations.
https://medium.com/techlog/consistent-hashing-with-bounded-loads-using-a-red-black-tree-b5aaf0d8540f


Git Used Merkle Tree for Diff checking and commiting.
https://www.geeksforgeeks.org/introduction-to-merkle-tree/
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
CAP Theorem or PACELC:
It says that if there is partition then Availability or Consistency will take a hit.
If 'E' ELSE ( not partition) then system will be available by default, and in that case Latency or Consistency will take a hit.
-> If system has low latency then it will not be eventual consistent and if system is eventual consistent then it will have high latency.( this is case of no partition).

The PACELC theorem is an extension to the CAP theorem, both of which provide frameworks for understanding trade-offs in distributed systems. Here's a breakdown:

CAP Theorem:

C: Consistency - All nodes in the system have the same data at the same time.
A: Availability - Every request receives a response, regardless of success or failure.
P: Partition Tolerance - The system continues to operate even when network partitions occur.
The CAP theorem states that only two of these properties can be achieved simultaneously in a distributed system. In simpler terms, you can have two out of these three:

Highly consistent but potentially unavailable: Requires waiting for all nodes to synchronize before responding to requests, which can be slow during partitions.
Highly available but potentially inconsistent: Allows reads and writes even during partitions, but data might be outdated across different nodes.
Partition tolerant but not necessarily consistent or available: Works during partitions but may exhibit either inconsistency or unavailability depending on the chosen approach.
PACELC Theorem:

The PACELC theorem builds on the CAP theorem by introducing another factor:

L: Latency - The time it takes for a request to be processed and a response to be received.
PACELC states that while the CAP trade-off applies during network partitions, even in normal operation, there's a trade-off between latency and consistency. Achieving low latency often involves relaxing consistency guarantees, such as using eventual consistency models where updates eventually propagate across all nodes but may not be immediately reflected everywhere.

In summary:

Both CAP and PACELC help understand the limitations and trade-offs in distributed systems regarding data consistency, availability, and now, latency.
PACELC expands on CAP by showing that even without partitions, achieving low latency may come at the cost of some consistency.
Choosing the right balance between these properties depends on the specific needs and requirements of your distributed system.
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
n high-level system design, the client request typically lands on an API Gateway, which acts as the single entry point for all external interactions with the system. Here's how it works:

1. Client Request: The client sends a request, usually through an HTTP API call, to the API Gateway. This request can originate from various sources like web browsers, mobile apps, or other systems.

2. API Gateway: The API Gateway performs several tasks:

* **Authentication and Authorization:** It validates the client's identity and ensures they have permission to access the requested resources.
* **Routing:** It routes the request to the appropriate backend service based on pre-defined rules (e.g., URL path, headers).
* **API Management:** It performs functions like rate limiting, throttling, and versioning to manage API traffic effectively.
* **Protocol Translation:** It may convert the client's request format (e.g., JSON) to the format expected by the backend service.
3. Web API & Load Balancer:

* **Web API:** The routed request lands on the designated web API, which is a specific application component responsible for handling the request's logic. This might involve database interaction, data processing, and generating a response.
* **Load Balancer:** If the system has multiple web APIs, a load balancer distributes incoming traffic among them to ensure even utilization and prevent overloading any single server.
Overall Flow:

Client -> API Gateway -> Load Balancer (optional) -> Web API -> Database/Services (optional) -> Response
Benefits:

Centralized entry point: simplifies security and management.
Flexibility: API Gateway can handle diverse clients and protocols.
Scalability: Load balancer distributes traffic for improved performance.
Decoupling: Web APIs are independent, allowing for easier development and maintenance.
Additional Considerations:

Different types of API Gateways exist (e.g., open-source, managed).
The specific design depends on the system's complexity and needs.
Security measures like encryption and access control are crucial.
I hope this explanation clarifies where client requests land and how API gateways, web APIs, and load balancers play a role in high-level system design. Feel free to ask if you have further questions!
------------------------------------------------------------------------------------------------------------------
Failover is the ability to seamlessly and automatically switch to a reliable backup system. Either redundancy or moving into a standby operational mode when a primary system component fails should achieve failover and reduce or eliminate negative user impact.
------------------------------------------------------------------------------------------------------------------
A WebSocket is a communication protocol that provides full-duplex communication channels over a single TCP connection. It enables real-time, event-driven communication between a client and a server.

Drawbacks of Websocket:
1. Some older browsers does not support, so application will not work in that.
2. Some Proxy and Firewall does not support or block websockets.
3. Stateful Nature: Unlike traditional HTTP, which is stateless, WebSockets are stateful. This means that the server needs to 
   maintain the connection state for each client, leading to increased memory usage and potential scalability challenges.
4. Security Considerations: With the persistent connection established by WebSockets, there is a need for proper security measures 
   to protect against potential vulnerabilities.
